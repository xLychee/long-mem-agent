# Experiment: Local Llama 3.2 via Ollama
# Prerequisites: 
#   1. Install Ollama: https://ollama.ai
#   2. Pull model: ollama pull llama3.2
#   3. Ensure Ollama is running
# Usage: python scripts/run_experiment.py --config configs/ollama-llama.yaml

model:
  name: llama3.2
  provider: ollama
  base_url: http://localhost:11434/v1
  temperature: 0.0
  max_tokens: 256

use_rag: true

retriever:
  model_name: all-MiniLM-L6-v2
  top_k: 5
  chunk_size: 5
  chunk_overlap: 2
  use_observations: false
  use_summaries: false

evaluation:
  experiment_name: llama3.2-local
  output_dir: ./results
  save_predictions: true
  tags:
    - local
    - ollama
    - llama

